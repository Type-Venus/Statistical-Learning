# Statistical-Learning
Final project "1-NN"
This code serves as the illustrative proof of the theorem 19.3 from the Shalev-Shwartz and Ben-David(2014) textbook. It starts with a few intuitive examples for the dimensionalities up to 3 and sample size up to 21 and proceeded by the more general graphs of the error's behavior in case of larger values of d and m as well as with another data distribution. It concludes by showing that the theory was correct predicting that the error boundary will have exponential decay form. 
Matplot library was used to draw the diagrams, numpy - for working with vectors and matrices, random - to generate a certain amount of points in accordance to the distribution and interval known in advance, math - to introduce some calculus functions, sklearn - to employ machine learning approach for 1-NN by splitting the data, train on it and test the results. The error value was chosen based on the multiple compilations of that piece of program for a different number of err's, but, generally speaking, it is also possible to nest one more cycle that would be updated in a geometrical fashion in order to see how errors behave with respect to the error's perturbations. Nevertheless, since it is about to complicate the program already having 3 loops, and the inverse proportional relation between the step size and Lipchitz constant c is not so obvious, it was decided to omit such an idea. The same reasoning was applied to the possibility of inclusion cycle for a number of distributions defined on [0,1], for example, Poisson, truncated normal, or Dirichlet.
It is easy to see how the graphs change when assigning the new values to d and m. Now they are set to 7 and 11, correspondingly, but you can experiment by substituting for the greater numbers. Similarly, if you type standard_normal or some other suitable operators instead of the current exponential in x, you will observe the picture for diverse underlying distributions. 
